/**
 * Represents a message in a conversation with an LLM
 */
export interface LLMMessage {
  /** The role of the message sender */
  role: 'system' | 'user' | 'assistant' | 'tool';
  
  /** The content of the message */
  content: string;
  
  /** Optional name of the message sender (for tool calls) */
  name?: string;
  
  /** Tool calls made by the model */
  tool_calls?: Array<{
    /** Unique identifier for the tool call */
    id: string;
    /** Type of the tool call */
    type: 'function';
    /** The function that the model called */
    function: {
      /** Name of the function to call */
      name: string;
      /** Arguments to call the function with */
      arguments: string;
    };
  }>;
  
  /** For tool responses, the tool call ID this message is responding to */
  tool_call_id?: string;
}

/**
 * Configuration for making a request to an LLM
 */
export interface LLMRequest {
  /** ID of the model to use */
  model: string;
  
  /** The messages to generate a response for */
  messages: LLMMessage[];
  
  /** 
   * Controls randomness (0-2). 
   * Lower values make output more deterministic.
   * @default 0.7
   */
  temperature?: number;
  
  /** 
   * Maximum number of tokens to generate.
   * @default 2048
   */
  max_tokens?: number;
  
  /** 
   * Controls diversity via nucleus sampling (0-1).
   * 0.5 means half of all likelihood-weighted options are considered.
   * @default 1
   */
  top_p?: number;
  
  /** 
   * Penalty for repeating tokens (-2.0 to 2.0).
   * @default 0
   */
  frequency_penalty?: number;
  
  /** 
   * Penalty for new tokens based on their existence in the text so far (-2.0 to 2.0).
   * @default 0
   */
  presence_penalty?: number;
  
  /** 
   * Whether to stream back partial progress.
   * @default false
   */
  stream?: boolean;
  
  /** 
   * Up to 4 sequences where the API will stop generating further tokens.
   */
  stop?: string | string[];
  
  /** 
   * List of available tools the model may call.
   */
  tools?: Array<{
    /** The type of the tool (currently only 'function' is supported) */
    type: 'function';
    /** The function definition */
    function: {
      /** The name of the function */
      name: string;
      /** A description of what the function does */
      description?: string;
      /** Parameters the function accepts */
      parameters: Record<string, unknown>;
    };
  }>;
  
  /** 
   * Controls which (if any) function is called by the model.
   * 'none' means the model will not call a function.
   * 'auto' means the model can pick between generating a message or calling a function.
   * Or specify a function to call: `{ type: 'function', function: { name: 'my_function' } }`
   */
  tool_choice?: 'none' | 'auto' | { type: 'function'; function: { name: string } };
  
  /** 
   * Specifies the format that the model must output.
   * Set to `{ type: 'json_object' }` to enable JSON mode.
   */
  response_format?: {
    /** Must be 'json_object' */
    type: 'json_object';
  };
}

/**
 * Response from an LLM API call
 */
export interface LLMResponse {
  /** A unique identifier for the completion */
  id: string;
  
  /** The model used for the completion */
  model: string;
  
  /** The list of completion choices */
  choices: Array<{
    /** The index of the choice in the list of choices */
    index: number;
    
    /** The message generated by the model */
    message: LLMMessage;
    
    /** The reason the model stopped generating tokens */
    finish_reason: 'stop' | 'length' | 'content_filter' | 'tool_calls' | 'function_call' | null;
    
    /** For streaming responses, partial message deltas */
    delta?: Partial<LLMMessage>;
  }>;
  
  /** Token usage statistics */
  usage: {
    /** Number of tokens in the prompt */
    prompt_tokens: number;
    
    /** Number of tokens in the completion */
    completion_tokens: number;
    
    /** Total number of tokens used in the request */
    total_tokens: number;
  };
  
  /** The Unix timestamp (in seconds) of when the completion was created */
  created: number;
  
  /** The object type, e.g., 'chat.completion' */
  object?: string;
  
  /** The organization this request belongs to */
  organization?: string;
  
  /** The ID of the request, if available */
  request_id?: string;
}

/**
 * Represents a chunk of a streaming response from an LLM
 */
export interface LLMStreamChunk {
  /** A unique identifier for the completion */
  id: string;
  
  /** The model used for the completion */
  model: string;
  
  /** The list of completion choices */
  choices: Array<{
    /** The index of the choice in the list of choices */
    index: number;
    
    /** The delta content of the message */
    delta: Partial<LLMMessage>;
    
    /** The reason the model stopped generating tokens, if applicable */
    finish_reason?: 'stop' | 'length' | 'content_filter' | 'tool_calls' | 'function_call' | null;
  }>;
  
  /** The Unix timestamp (in seconds) of when the completion was created */
  created?: number;
  
  /** The object type, e.g., 'chat.completion.chunk' */
  object?: string;
}

/**
 * Configuration options for an LLM provider client
 */
export interface LLMProviderConfig {
  /** API key for authentication */
  apiKey?: string;
  
  /** Base URL for the API endpoint */
  baseUrl?: string;
  
  /** Request timeout in milliseconds */
  timeout?: number;
  
  /** Maximum number of retry attempts for failed requests */
  maxRetries?: number;
  
  /** Delay between retry attempts in milliseconds */
  retryDelay?: number;
  
  /** Custom headers to include in requests */
  customHeaders?: Record<string, string>;
  
  /** Version of the API to use */
  apiVersion?: string;
  
  /** Organization ID for usage tracking */
  organization?: string;
  
  /** Project ID for usage tracking */
  project?: string;
  
  /** Whether to enable debug logging */
  debug?: boolean;
}

/**
 * Result of executing a prompt with an LLM
 */
export interface LLMExecutionResult {
  /** The generated content */
  content: string;
  
  /** Total number of tokens used (prompt + completion) */
  tokensUsed: number;
  
  /** Time taken to execute the request in milliseconds */
  executionTimeMs: number;
  
  /** Estimated cost in USD, if available */
  cost?: number;
  
  /** The model used for generation */
  model: string;
  
  /** The provider that handled the request */
  provider: string;
  
  /** The finish reason for the completion */
  finishReason?: 'stop' | 'length' | 'content_filter' | 'tool_calls' | 'function_call' | null;
  
  /** Any metadata associated with the execution */
  metadata?: Record<string, unknown>;
  
  /** The raw response from the provider, if available */
  rawResponse?: unknown;
}

/**
 * Information about an LLM model
 */
export interface LLMModelInfo {
  /** Unique identifier for the model */
  id: string;
  
  /** Display name of the model */
  name: string;
  
  /** The provider of the model */
  provider: string;
  
  /** Maximum context length in tokens */
  context_length?: number;
  
  /** Pricing information in USD per 1K tokens */
  pricing?: {
    /** Cost per 1K prompt tokens */
    prompt: number;
    
    /** Cost per 1K completion tokens */
    completion: number;
  };
  
  /** When the model was created or made available */
  created_at?: number;
  
  /** Whether the model supports function calling */
  supports_function_calling?: boolean;
  
  /** Whether the model supports streaming */
  supports_streaming?: boolean;
  
  /** The model's capabilities */
  capabilities?: {
    /** Whether the model supports function calling */
    function_calling?: boolean;
    
    /** Whether the model supports vision */
    vision?: boolean;
    
    /** Whether the model supports parallel function calling */
    parallel_function_calling?: boolean;
  };
  
  /** Any additional metadata */
  metadata?: Record<string, unknown>;
}

/**
 * Supported LLM provider types
 */
export type LLMProviderType = 
  /** OpenAI API compatible providers */
  | 'openai' 
  
  /** OpenRouter API */
  | 'openrouter' 
  
  /** LM Studio local inference */
  | 'lmstudio' 
  
  /** Ollama local models */
  | 'ollama' 
  
  /** Anthropic Claude models */
  | 'anthropic' 
  
  /** Custom provider with custom implementation */
  | 'custom' 
  
  /** Google's Gemini models */
  | 'google' 
  
  /** Mistral AI models */
  | 'mistral' 
  
  /** Cohere models */
  | 'cohere';

/**
 * Standard finish reasons for LLM completions
 */
export type FinishReason = 
  /** The model hit a natural stop point or provided a complete response */
  | 'stop' 
  
  /** The model reached the maximum token limit */
  | 'length' 
  
  /** Content was omitted due to content filters */
  | 'content_filter' 
  
  /** The model called a tool/function */
  | 'tool_calls' 
  
  /** @deprecated Use tool_calls instead */
  | 'function_call' 
  
  /** The model provided an empty response */
  | null;